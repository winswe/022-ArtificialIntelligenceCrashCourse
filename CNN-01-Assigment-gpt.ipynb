{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb328182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5d8fa0",
   "metadata": {},
   "source": [
    "## Define CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5e75b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FaceCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 256)  # for 224x224 input\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff2e9f",
   "metadata": {},
   "source": [
    "## Dataset & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0861275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes -> Train: 1793, Val: 384, Test: 385\n",
      "Classes: ['Akshay Kumar', 'Alexandra Daddario', 'Alia Bhatt', 'Amitabh Bachchan', 'Andy Samberg', 'Anushka Sharma', 'Billie Eilish', 'Brad Pitt', 'Camila Cabello', 'Charlize Theron', 'Claire Holt', 'Courtney Cox', 'Dwayne Johnson', 'Elizabeth Olsen', 'Ellen Degeneres', 'Henry Cavill', 'Hrithik Roshan', 'Hugh Jackman', 'Jessica Alba', 'Kashyap', 'Lisa Kudrow', 'Margot Robbie', 'Marmik', 'Natalie Portman', 'Priyanka Chopra', 'Robert Downey Jr', 'Roger Federer', 'Tom Cruise', 'Vijay Deverakonda', 'Virat Kohli', 'Zac Efron']\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load all images (class = folder name)\n",
    "full_dataset = datasets.ImageFolder(\"face_dataset/Original Images\", transform=transform)\n",
    "\n",
    "# Split into train / val / test (70/15/15)\n",
    "train_ratio, val_ratio, test_ratio = 0.7, 0.15, 0.15\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size   = int(val_ratio * total_size)\n",
    "test_size  = total_size - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Dataset sizes -> Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "print(\"Classes:\", full_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ef9cd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ebcc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3.8608, Val Acc: 5.73%\n",
      "Epoch 2/10, Loss: 3.2912, Val Acc: 9.38%\n",
      "Epoch 3/10, Loss: 2.8728, Val Acc: 17.19%\n",
      "Epoch 4/10, Loss: 2.4291, Val Acc: 15.62%\n",
      "Epoch 5/10, Loss: 1.9673, Val Acc: 19.27%\n",
      "Epoch 6/10, Loss: 1.2403, Val Acc: 21.09%\n",
      "Epoch 7/10, Loss: 0.3605, Val Acc: 20.05%\n",
      "Epoch 8/10, Loss: 0.0549, Val Acc: 20.31%\n",
      "Epoch 9/10, Loss: 0.0111, Val Acc: 22.14%\n",
      "Epoch 10/10, Loss: 0.0040, Val Acc: 22.40%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FaceCNN(num_classes=len(full_dataset.classes)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss/len(train_loader):.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec6deb0",
   "metadata": {},
   "source": [
    "## Final Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f85a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"âœ… Final Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03c7fa",
   "metadata": {},
   "source": [
    "## Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        _, pred = torch.max(output, 1)\n",
    "    return full_dataset.classes[pred.item()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
